{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesamiento del Lenguaje Natural\n",
    "===\n",
    "Lab 3.1 - Implementación personalizada de CountVectorizer al Español\n",
    "===\n",
    "Alumno: Gonzalo Bertinat\n",
    "\n",
    "Legajo: 160.615-3\n",
    "\n",
    "Curso: K3572\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "Enunciado:\n",
    "\n",
    "_Subir a tu github una implementación personalizada de NLTK para CountVectorizer\n",
    "que haga steam y stopwords del idioma español y dos ejemplos de oraciones usando tu clase.\n",
    "También importá un corpus como 20_Newsdataset pero que esté en español. Qué\n",
    "corpus poner, queda a tu criterio!_\n",
    "\n",
    "Bien, para resolver el enunciado necesitamos nuestra propia implementación del CountVectorizer, por lo que vamos a crear una clase que herede de CountVectorizer como en el Lab 3.\n",
    "\n",
    "Antes que nada estaría bueno saber algunas de las Stop Words que existen en español.\n",
    "Si hacemos como se nos enseño en el lab deberíamos ejecutar el siguiente código:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not a built-in stop list: spanish",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-34852837aa21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'spanish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mget_stop_words\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;34m\"\"\"Build or fetch the effective stop words list\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_check_stop_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_stop_words_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_stop_list\u001b[0;34m(stop)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"not a built-in stop list: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not a built-in stop list: spanish"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'spanish')\n",
    "\n",
    "vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ups, nos encontramos con un error, si prestamos atención abajo nos dice que:\n",
    "\n",
    "_\"ValueError: not a built-in stop list: spanish\"_\n",
    "\n",
    "Esto sucede porque 'spanish' no está contemplado como valor posible al instanciar un CountVectorizer. \n",
    "Si revisamos la documentación de SciKit learn: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "Vemos que nos dice en los parámetros del constructor el detalle del parámetro **stop_words**:\n",
    "\n",
    "_\"stop_words : string {‘english’}, list, or None (default)\"_\n",
    "\n",
    "La documentación oficial nos comenta que stop_words puede valer el string literal \"english\" (que es como se venía haciendo hasta ahora), una lista o None (sería sin enviarle el valor en el constructor).\n",
    "\n",
    "Como \"spanish\" no es el string \"english\" ni es una lista, no podemos hacerlo como en el código escrito recién.\n",
    "\n",
    "La forma correcta será usando la opción del parámetro en forma de lista, y en esa lista enviaremos las palabras Stop Words del idioma español.\n",
    "\n",
    "Pero.. ¿de dónde las sacamos? De nuestro queridísimo NLTK.\n",
    "Con el código siguiente obtenemos las Stop Words del español y mostramos las primeras 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "print(spanish_stopwords[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí vamos a poder crear nuestra clase e instanciarla con idioma español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos Stem de NLTK para hacer stemming\n",
    "import nltk.stem\n",
    "\n",
    "# Importamos el CountVectorizer original para hacer una sobreescritura\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Creamos un stemmer de NLTK para idioma Español, en este caso sí está contemplado el string \"spanish\" !\n",
    "spanish_stemmer = nltk.stem.SnowballStemmer('spanish')\n",
    "\n",
    "# Definición de la clase, que hereda de CountVectorizer\n",
    "class StemmedSpanishCountVectorizer(CountVectorizer):\n",
    "    \n",
    "    # Redefinición del build_analyzer\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedSpanishCountVectorizer,self).build_analyzer()\n",
    "        # Aplicamos el Steammer de NLTK a la salida del build_analyzer\n",
    "        return lambda doc: (spanish_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instaciamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso enviamos de parámetro de stop_words la lista que obtuvimos recién\n",
    "stem_vectorizer = StemmedSpanishCountVectorizer(min_df=1, stop_words=spanish_stopwords)\n",
    "stem_analyze = stem_vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, el enunciado nos pide que usemos dos oraciones de ejemplo, y luego un corpus en español.\n",
    "\n",
    "Vamos con lo primero: declaremos dos oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase1 = \"A Juan le gusta mucho salir a correr.\"\n",
    "frase2 = \"La comida preferida de María es la pizza.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a obtener el steam (raíz) de las palabras de cada frase ignorando las Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juan\n",
      "gust\n",
      "sal\n",
      "corr\n"
     ]
    }
   ],
   "source": [
    "X = stem_analyze(frase1)\n",
    "for stem in X:\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "com\n",
      "prefer\n",
      "mar\n",
      "pizz\n"
     ]
    }
   ],
   "source": [
    "Y = stem_analyze(frase2)\n",
    "for stem in Y:\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora importemos un corpus en Español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraigamos las primeras 70 palabras, para poder visualizar y obtener dos oraciones de este corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El',\n",
       " 'grupo',\n",
       " 'estatal',\n",
       " 'Electricité_de_France',\n",
       " '-Fpa-',\n",
       " 'EDF',\n",
       " '-Fpt-',\n",
       " 'anunció',\n",
       " 'hoy',\n",
       " ',',\n",
       " 'jueves',\n",
       " ',',\n",
       " 'la',\n",
       " 'compra',\n",
       " 'del',\n",
       " '51_por_ciento',\n",
       " 'de',\n",
       " 'la',\n",
       " 'empresa',\n",
       " 'mexicana',\n",
       " 'Electricidad_Águila_de_Altamira',\n",
       " '-Fpa-',\n",
       " 'EAA',\n",
       " '-Fpt-',\n",
       " ',',\n",
       " 'creada',\n",
       " 'por',\n",
       " 'el',\n",
       " 'japonés',\n",
       " 'Mitsubishi_Corporation',\n",
       " 'para',\n",
       " 'poner_en_marcha',\n",
       " 'una',\n",
       " 'central',\n",
       " 'de',\n",
       " 'gas',\n",
       " 'de',\n",
       " '495',\n",
       " 'megavatios',\n",
       " '.',\n",
       " 'Una',\n",
       " 'portavoz',\n",
       " 'de',\n",
       " 'EDF',\n",
       " 'explicó',\n",
       " 'a',\n",
       " 'EFE',\n",
       " 'que',\n",
       " 'el',\n",
       " 'proyecto',\n",
       " 'para',\n",
       " 'la',\n",
       " 'construcción',\n",
       " 'de',\n",
       " 'Altamira_2',\n",
       " ',',\n",
       " 'al',\n",
       " 'norte',\n",
       " 'de',\n",
       " 'Tampico',\n",
       " ',',\n",
       " 'prevé',\n",
       " 'la',\n",
       " 'utilización',\n",
       " 'de',\n",
       " 'gas',\n",
       " 'natural',\n",
       " 'como',\n",
       " 'combustible',\n",
       " 'principal']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cess_esp.words()[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"anunció hoy, jueves, la compra del 51_por_ciento de la empresa mexicana\"\n",
    "cess_frase1_words = cess_esp.words()[7:21]\n",
    "# \"Una portavoz de EDF explicó a EFE que el proyecto para la construcción de Altamira_2, al norte de Tampico, prevé la utilización de gas natural como combustible principal\"\n",
    "cess_frase2_words = cess_esp.words()[40:70]\n",
    "\n",
    "# Convertimos las listas de palabras a una cadena:\n",
    "cess_frase1 = \" \".join(str(x) for x in cess_frase1_words)\n",
    "cess_frase2 = \" \".join(str(x) for x in cess_frase2_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anunc\n",
      "hoy\n",
      "juev\n",
      "compr\n",
      "51_por_cient\n",
      "empres\n",
      "mexican\n",
      "electricidad_aguila_de_altamir\n"
     ]
    }
   ],
   "source": [
    "# Hacemos el stemming\n",
    "X = stem_analyze(cess_frase1)\n",
    "for stem in X:\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anunc\n",
      "hoy\n",
      "juev\n",
      "compr\n",
      "51_por_cient\n",
      "empres\n",
      "mexican\n",
      "electricidad_aguila_de_altamir\n"
     ]
    }
   ],
   "source": [
    "# Lo mismo con la segunda frase\n",
    "Y = stem_analyze(cess_frase1)\n",
    "for stem in Y:\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos concluir que el steam en español de SnowballStemmer hace el steamming aplicando el método del truncado, quita letras del final, asumiendo que las primeras letras son la raíz (steam) de la palabra."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
