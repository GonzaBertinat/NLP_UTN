{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesamiento del Lenguaje Natural\n",
    "===\n",
    "Lab 3 - Procesando Texto y usando SciKit-learn\n",
    "===\n",
    "Alumno: Gonzalo Bertinat\n",
    "\n",
    "Legajo: 160.615-3\n",
    "\n",
    "Curso: K3572\n",
    "\n",
    "============================================\n",
    "\n",
    "El uso de SciKit-learn\n",
    "===\n",
    "\n",
    "SciKit-learn es una biblioteca de Python para Machine Learning. Sirve para el procesar textos, mediante el agrupamiento y la clasificación. Incluye la tokenización, el conteo de palabras y el steamming.\n",
    "\n",
    "Para poder utilizarlo debemos importarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos scikit learn\n",
    "import sklearn\n",
    "# También importamos numPy, lo vamos a necesitar luego\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo importante para destacar, es que SciKit Learn interactúa bien con paquetes de visualización como Pyplot.\n",
    "\n",
    "Veamos un ejemplo en pyplot para visualizar una matriz bidimensonal que representa a una función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importamos pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Usamos numPy para armar un array tomando puntos x,y en forma de vector [x,y]\n",
    "data = np.array([[1,2], [2,3], [3,4], [4,5], [5,6]])\n",
    "# Tomamos los valores de X y de Y\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "\n",
    "# Armamos el gráfico\n",
    "plt.scatter(x,y)\n",
    "plt.grid(True)\n",
    "# Lo mostramos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento de Texto con SciKit-learn\n",
    "===\n",
    "\n",
    "Muchas de las aplicaciones de análisis de texto requieren que tomemos un texto, lo tokenizemos, y usemos los tokens como features, posiblemente después de eliminar palabras con lematización y stop words.\n",
    "\n",
    "Afortunadamente, no es necesario escribir todo este código, ya que SciKit-learn nos hace todo más fácil.\n",
    "Existe una clase CountVectorizer que se encarga de esto.\n",
    "\n",
    "Vamos a ver un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la clase\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Instanciamos\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar documentos a procesar vamos a utilizar una colección.\n",
    "Por ser un primer ejemplo, usamos una lista de cadenas como documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\"How to format my hard disk\", \"Hard disk format problems\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con nuestra instancia, podemos extraer una bolsa de palabras de los documentos. Para eso usamos el método fit_transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_transform extrajo siete features de los dos \"documentos\", si queremos verlas usamos get_feature_names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También, podemos ver cuántas veces cada una de estas siete features se produce en los dos documentos haciendo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto devuelve una matriz de dos filas, una por documento. Cada fila tiene siete elementos. Cada elemento representa el número de veces que una feature se produjo en ese documento.\n",
    "\n",
    "Entonces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta última operación obtenemos el vector sólo para el primer documento \"How to format my hard disk\", que contiene todas las features, excepto \"problems\".\n",
    "\n",
    "Ahora si queremos la cantidad de veces que aparece la palabra 'hard' en el segundo documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()[1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ver cómo funciona esta librería pero con una verdadera colección de documentos.\n",
    "\n",
    "Vamos a utilizar el dataset '20 Newsgroups', que es una colección de alrededor de 20.000 documentos procedentes de 20 grupos de noticias diferentes, que se usa comúnmente en experimentos de clasificación y agrupación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Para no extender demasiado el lab, usaremos sólo un subconjunto de los documentos\n",
    "# Más precisamente, 4 categorías\n",
    "categories = ['alt.atheism','soc.religion.christian','comp.graphics','sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos importar los documentos pertenecientes a las categorías de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los archivos quedan cargados en el atributo 'data' del objeto twenty_train. \n",
    "\n",
    "Vamos a crear un nuevo objeto CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como hicimos antes, importamos y luego instanciamos\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer lo mismo de antes. Usaremos la función fit_transform para tokenizar los documentos, identificar palabras relevantes, y crear una representación vectorial en que las palabras son features y el valor de estas features es el número de ocurrencias de cada palabra en un documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocamos a fit_transform\n",
    "train_counts = vectorizer.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, si queremos ver la frecuencia de la palabra 'algorithm' dentro del subconjunto que elegimos de la colección 20Newsgroups haremos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver cuántos features fueron extraídos, usamos la función get_feature_names() de antes, pero pedimos su longitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35788"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recordamos: con len() pedimos la longitud de una lista\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punto y aparte, la clase CountVectorizer también puede hacer más procesamiento previo de una colección que simples tokenizaciones.\n",
    "\n",
    "Una importante etapa de preprocesamiento adicional es la eliminación de \"stop words\", que son las palabrás más comúnes y puntuaciones de algún idioma.\n",
    "\n",
    "Podemos hacer esto de la siguiente forma:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos la clase, pero pasandole de parámetro el idioma\n",
    "# que queremos que tenga en cuenta para ignorar Stop Words.\n",
    "Vectorizer = CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver qué palabras son Stop Words, hacemos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usamos el método get_stop_words()\n",
    "# En este caso lo ordenamos, y luego pedimos las primeras 20 posiciones\n",
    "sorted(Vectorizer.get_stop_words())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisieramos hacer stemming (obtener la palabra raíz) y un pre-procesamiento más avanzado, necesitamos complementar a SciKit-learn con NLTK.\n",
    "\n",
    "Vamos a ello...\n",
    "\n",
    "Pre-procesamiento más avanzado con NLTK\n",
    "===\n",
    "\n",
    "NLTK es una biblioteca que ya usamos en los labs anteriores.\n",
    "\n",
    "Es compatible con la mayoria de los tipos de procesamiento previo, y es una biblioteca mucho más grande que SciKit-learn, ya que incluye su propia implementación de muchos algoritmos de aprendizaje automático.\n",
    "\n",
    "Para importarlo, como siempre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El steamming en NLTK incluye implementaciones de varios algoritmos muy conocidos y utilizados, como el Porter Stemmer y el Lancaster Stemmer.\n",
    "\n",
    "Vamos a ver como crear un stemmer de Inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de instanciarlo, pidamos la raíz de algunas palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con el método stem, nos devuelve el steam de cada palabra\n",
    "s.stem(\"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"loving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otros tipos de pre-procesamiento de NLTK incluyen implementaciones de muchos de los módulos de procesamiento previo y analizadores sintácticos que discutimos en las clases:\n",
    "- Identificadores de idioma\n",
    "- Tokenizers para varios idiomas\n",
    "- Divisores de oraciones\n",
    "- POS taggers\n",
    "- Chunkers\n",
    "- Parsers\n",
    "\n",
    "Además, NLTK incluye implementaciones para NER (Named Entity Recognition), análisis de sentimientos y extracción de información de redes sociales.\n",
    "\n",
    "Por ejemplo, el código siguiente nos permite tokenizar una frase y luego usarla de input en un POS tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos el tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenizamos una frase\n",
    "text = word_tokenize(\"And now for something completely different\")\n",
    "# Ejecutamos el POS tagger\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La integración del Steammer de NLTK con CountVectorizer de SciKit-learn:\n",
    "===\n",
    "\n",
    "El steammer de NLTK puede ser utilizado antes de alimentar a CountVectorizer de SciKit, obteniendo un índice más compacto.\n",
    "\n",
    "Es decir, convertimos todas las palabras de los documentos primero a su raíz (steam) y luego van como input del CountVectorizer\n",
    "\n",
    "Una forma de hacer esto es definir una clase nueva que herede de CountVectorizer y redefinir el método build_analyzer(). Este método toma un string como entrada y retorna una lista de tokens.\n",
    "\n",
    "Veamos como trabaja originalmente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john', 'bought', 'carrots', 'potatoes']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"John bought carrots and potatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: Vemos como se ignoró la palabra \"and\" ya que esta vez tuvimos en cuenta las Stop Words!\n",
    "\n",
    "Vamos a crear la clase y a sobreescribir el método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# Definición de la clase\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        # Aplicamos el Steammer de NLTK a la salida del build_analyzer\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora instanciamos nuestra clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "stem_analyze = stem_vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos la lista de tokens que nos retonra nuestro build_analyzer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n",
      "bought\n",
      "carrot\n",
      "potato\n"
     ]
    }
   ],
   "source": [
    "Y = stem_analyze(\"John bought carrots and potatoes\")\n",
    "for token in Y:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver como \"carrots\" y \"potatoes\" pasaron a su versión en singular, \"carrot\" y \"potato\" respectivamente, es decir, sus steams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usemos este nuevo Vectorizer personalizado para extraer features para el subconjunto del dataset 20_Newsgroups que consideramos antes. Se espera tener un número menor de features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26888\n"
     ]
    }
   ],
   "source": [
    "# Importamos el dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Definimos las categorías para el subset\n",
    "categories = [\"alt.atheism\",\"soc.religion.christian\",\"comp.graphics\",\"sci.med\"]\n",
    "# Pedimos un subset\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "# Aplicamos el dataset al Stem Vectorizer\n",
    "stem_vectorizer.fit_transform(twenty_train.data)\n",
    "# Pedimos la cantidad de features obtenidos\n",
    "train_counts = len(stem_vectorizer.get_feature_names())\n",
    "# Mostramos esa cantidad\n",
    "print(train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el número se redujo ampliamente respecto de  las 35788 features obtenidas antes sin el Steamming. Esto se debe a que varias palabras pueden tener como raíz la misma palabra, por lo que se tiene en cuenta en la implementación personalizada solo a la raíz y no a sus palabras derivadas.\n",
    "\n",
    "Implementación personalizada de NLTK para CountVectoirzer que haga steam y stopwords del idioma español y dos ejemplos de oracionetrain_counts s usando esa clase:\n",
    "LINK: --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
