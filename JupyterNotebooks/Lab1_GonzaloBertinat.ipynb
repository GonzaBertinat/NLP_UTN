{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesamiento del Lenguaje Natural\n",
    "===\n",
    "Lab 1 - NLTK y Contar Palabras Corpora\n",
    "===\n",
    "**Alumno:** Gonzalo Bertinat\n",
    "\n",
    "**Legajo:** 160.615-3\n",
    "\n",
    "**Curso:** K3572\n",
    "\n",
    "**Probando sus conocimientos**\n",
    "===\n",
    "\n",
    "Veamos la primer pregunta del cuestionario...\n",
    "\n",
    "**1. ¿Cuál es el número de tokens en Moby Dick?**\n",
    "\n",
    "Antes de ponernos a calcular dicho número, vamos a importar lo que consideremos necesario...\n",
    "\n",
    "Primero, hay que importar el texto de Moby Dick! \n",
    "Para eso, como se nos explicó a lo largo del instructivo del Lab debemos realizar lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Importamos los textos existentes desde NLTK\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como ya se aclaró en el instructivo, Moby Dick se corresponde con el text1.\n",
    "# Guardamos en una variable los tokens...\n",
    "moby_dick_tokens = text1.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora uno estaría tentado a poder contestar la pregunta, ¿no?\n",
    "\n",
    "Acaso no tenemos los tokens? Tranquilamente podríamos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(moby_dick_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y listo, ¿verdad?\n",
    "\n",
    "Bueno, la realidad es que **esto está mal.** \n",
    "\n",
    "El enunciado nos aclara antes de preguntarnos cualquier cosa que:\n",
    "    \n",
    "**Remueva las puntuaciones y convierta a minúsculas (lower case) antes de hacer las cuentas.**\n",
    "\n",
    "Mejor hagamos caso..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos re para poder utilizar expresiones regulares...\n",
    "from nltk import re\n",
    "# Mediante re.search(\"\\w\",word) matcheamos solo aquellos tokens que son alfanuméricos.\n",
    "# Con el for iteramos sobre todos los tokens de moby_dick_tokens y los convertimos a minúsculas con lower()\n",
    "moby_dick_tokens_nopunct = [word.lower() for word in moby_dick_tokens if re.search(\"\\w\",word)]                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genial! Ahora sí tenemos los tokens como los pide el enunciado.\n",
    "\n",
    "Estamos en condiciones de pedir la cantidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de tokens de Moby Dick es: 218621\n"
     ]
    }
   ],
   "source": [
    "cantidad_tokens_MD = len(moby_dick_tokens_nopunct)\n",
    "print(\"La cantidad de tokens de Moby Dick es:\", cantidad_tokens_MD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excelente!\n",
    "\n",
    "Veamos la siguiente:\n",
    "\n",
    "**2. ¿Cuál es el número de types en Moby Dick?**\n",
    "\n",
    "Bueno, para esto primero recordemos que los **types** no eran nada menos que los tokens pero quitando sus repeticiones, es decir, **el número de palabras únicas**.\n",
    "\n",
    "Afortunadamente, para quitar los tokens repetidos que tenemos guardados en la variable **moby_dick_tokens_nopunct** vamos a utilizar el set de Python, que justamente, crea una colección de elementos SIN REPETIR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_dick_types = set(moby_dick_tokens_nopunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos contestar la pregunta 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de Types en Moby Dick es: 17140\n"
     ]
    }
   ],
   "source": [
    "cantidad_types_MD = len(moby_dick_types)\n",
    "print(\"La cantidad de Types en Moby Dick es:\",cantidad_types_MD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Moby Dick type-token ratio**\n",
    "\n",
    "Ahora se nos pide este ratio... Pero, ¿qué es?\n",
    "\n",
    "Simplemente es dividir la cantidad de types por la cantidad de tokens. \n",
    "\n",
    "Lo que hace es darnos una idea de la diversidad léxica de un texto, es decir que tan diverso en palabras es en relación a la cantidad total de palabras que lo componen.\n",
    "\n",
    "Se nos pide el TTR (Type-Token Ratio) de Moby Dick, y afortunadamente de lo que ya hicimos antes tenemos guardados en variables los dos valores que necesitamos: la cantidad de tokens y la cantidad de types.\n",
    "\n",
    "Entonces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Type-Token ratio de Moby Dick es: 0.07840051962071347\n"
     ]
    }
   ],
   "source": [
    "moby_dick_TTR = cantidad_types_MD / cantidad_tokens_MD\n",
    "print(\"El Type-Token ratio de Moby Dick es:\",moby_dick_TTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. WSJ type-token ratio**\n",
    "\n",
    "Para esta tarea repetimos el mismo código que realizamos con Moby Dick, pero ahora con el texto de Wall Street Journal (WSJ).\n",
    "\n",
    "La única diferencia es que como el texto es otro, debemos pedir los tokens en este caso al texto 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de tokens en Wall Street Journal es 87608\n",
      "La cantidad de types en Wall Street Journal es: 11367\n",
      "El Type-Token Ratio de Wall Street Journal es: 0.129748424801388\n"
     ]
    }
   ],
   "source": [
    "# Pedimos los tokens\n",
    "WSJ_tokens = text7.tokens\n",
    "\n",
    "# Quitamos tokens de puntuación y convertimos a minúsculas \n",
    "WSJ_tokens_no_punct = [word.lower() for word in WSJ_tokens if re.search(\"\\w\",word)]\n",
    "\n",
    "# Cantidad de tokens final\n",
    "cantidad_tokens_WSJ = len(WSJ_tokens_no_punct)\n",
    "\n",
    "# Verificamos esa cantidad:\n",
    "print(\"La cantidad de tokens en Wall Street Journal es\",cantidad_tokens_WSJ)\n",
    "\n",
    "# Ahora armamos un set para eliminar repetidos y pedimos la cantidad para obtener la cantidad de types\n",
    "cantidad_types_WSJ = len(set(WSJ_tokens_no_punct))\n",
    "print(\"La cantidad de types en Wall Street Journal es:\",cantidad_types_WSJ)\n",
    "\n",
    "# Calculamos el TTR de WSJ \n",
    "WSJ_TTR = cantidad_types_WSJ/cantidad_tokens_WSJ\n",
    "print(\"El Type-Token Ratio de Wall Street Journal es:\",WSJ_TTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. ¿Cuál de los dos tiene más diversidad léxica?**\n",
    "\n",
    "Wall Street Journal con 0.129748424801388\n",
    "\n",
    "Pero para hacerlo más divertido y con código Python, mejor se lo preguntamos a él:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal tiene mayor diversidad léxica! Su Type-Token Ratio fue: 0.129748424801388\n"
     ]
    }
   ],
   "source": [
    "if moby_dick_TTR > WSJ_TTR:\n",
    "    print(\"Moby Dick tiene mayor diversidad léxica! Su Type-Token Ratio fue:\",moby_dick_TTR)\n",
    "elif WSJ_TTR > moby_dick_TTR:\n",
    "    print(\"Wall Street Journal tiene mayor diversidad léxica! Su Type-Token Ratio fue:\",WSJ_TTR)\n",
    "else:\n",
    "    print(\"Moby Dick y Wall Street Journal son igualmente diversos léxicamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. ¿Puede pensar una razón por por la cual ese corpus es más diverso que el otro?**\n",
    "\n",
    "Si tengo que responder en frío esta pregunta le haría caso a mi profesor respondiendo un **\"Depende\"**. Pero hay que razonar un poco según lo que se hizo hasta ahora en el lab.\n",
    "\n",
    "Haciendo un análisis rápido, puedo llegar a la siguiente conclusión:\n",
    "\n",
    "Wall Street Journal tiene menos cantidad de palabras, por lo que la probabilidad de que repita palabras\n",
    "es menor, y si se repiten menos las palabras, el ratio empezará a crecer, tendremos más types.\n",
    "Creo que es natural que cuanto más escribamos, más repetimos palabras de uso común, como por ejemplo las \n",
    "palabras \"el\", \"la\", \"si\", \"a\", etcétera. Por eso considero que Moby Dick tiene un ratio menor, pensar solamente\n",
    "en cuántas veces aparece la palabra \"Moby\", debe haber bastantes si de un personaje se trata.\n",
    " \n",
    "Espero que sea correcto, aunque insisto que depende mucho del texto que se esté analizando, un autor tiene tendencia a escribir de una misma forma, por lo que comenzará a repetir léxico y se hará menos diverso, eso también podría explicar que Moby Dick tenga menor ratio. Por otro lado Wall Street Journal contempla noticias, las cuales a priori deberían ser de muchos tópicos, incorporando muchas palabras distintas y dandole más ratio que a Moby Dick.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. ¿Cuál es el “Maximum Likelihood Estimate (MLE)” de la palabra “whale” (ballena) en Moby Dick?**\n",
    "\n",
    "Para responder esto, recordemos que el Maximum Likelihood Estimate de una palabra se podía calcular en Unigram como la cantidad de apariciones de esa palabra en el corpus dividida por la cantidad de palabras del corpus. \n",
    "\n",
    "En unigram, el MLE es equivalente a la probabilidad de que una palabra aparezca en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ MLE(wi)=\\frac{count(wi)}{N} $$\n",
    "\n",
    "Dejemos que Python se encargue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Maximum Likelihood Estimate de la palabra 'whale' en Moby Dick es: 0.005607878474620462\n",
      "Pmoby dick(“whale”) = 0.005607878474620462\n"
     ]
    }
   ],
   "source": [
    "# Con el método count() calculamos la cantidad de apariciones en el corpus\n",
    "cantidad_apariciones_whale_MD = moby_dick_tokens_nopunct.count(\"whale\")\n",
    "# Aplicamos la fórmula del MLE\n",
    "MLE_whale_moby_dick = cantidad_apariciones_whale_MD / cantidad_tokens_MD\n",
    "\n",
    "print(\"El Maximum Likelihood Estimate de la palabra 'whale' en Moby Dick es:\",MLE_whale_moby_dick)\n",
    "print(\"Pmoby dick(“whale”) =\",MLE_whale_moby_dick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. ¿Cuál es el MLE de “whale” en el corpus de WSJ?**\n",
    "\n",
    "Planteamos exactamente lo mismo que para Moby Dick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Maximum Likelihood Estimate de la palabra 'whale' en Wall Street Journal es: 0.0\n",
      "Pmoby dick(“whale”) = 0.0\n"
     ]
    }
   ],
   "source": [
    "cantidad_apariciones_whale_WSJ = WSJ_tokens_no_punct.count(\"whale\")\n",
    "MLE_whale_WSJ = cantidad_apariciones_whale_WSJ / cantidad_tokens_WSJ\n",
    "print(\"El Maximum Likelihood Estimate de la palabra 'whale' en Wall Street Journal es:\",MLE_whale_WSJ)\n",
    "print(\"Pmoby dick(“whale”) =\",MLE_whale_WSJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, repasando todas las respuestas:\n",
    "\n",
    "1. La cantidad de tokens de Moby Dick es: 218621\n",
    "2. La cantidad de Types en Moby Dick es: 17140\n",
    "3. El Type-Token ratio de Moby Dick es: 0.07840051962071347\n",
    "4. El Type-Token Ratio de Wall Street Journal es: 0.129748424801388\n",
    "5. Wall Street Journal tiene mayor diversidad léxica que Moby Dick\n",
    "6. Contestada ampliamente más arriba\n",
    "7. El Maximum Likelihood Estimate de la palabra 'whale' en Moby Dick es: 0.005607878474620462\n",
    "8. El Maximum Likelihood Estimate de la palabra 'whale' en Wall Street Journal es: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto fue todo amigos!\n"
     ]
    }
   ],
   "source": [
    "print(\"Esto fue todo amigos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIN DEL NOTEBOOK\n",
    "=="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
